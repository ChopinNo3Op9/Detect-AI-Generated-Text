{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To activate this environment, use\n",
    "$ conda activate tf\n",
    "\n",
    "To deactivate an active environment, use\n",
    "$ conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Steven\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import keras_nlp\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate what do you think of when I say that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ever since it was first invented, the automobi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Have you ever come back to school after a long...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imagine you receiving advice from someone, but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you waych the news chaces are you probelly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Title: The Pros and Cons of a Four-Day Work We...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>\\nWhether summer projects for students should ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Building self-esteem by accomplishing goals ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Title: Minister Winston's Selfless Acts of Kin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>As a grade 7 student, I do not have personal o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  generated\n",
       "0    Candidate what do you think of when I say that...          0\n",
       "1    Ever since it was first invented, the automobi...          0\n",
       "2    Have you ever come back to school after a long...          0\n",
       "3    Imagine you receiving advice from someone, but...          0\n",
       "4    If you waych the news chaces are you probelly ...          0\n",
       "..                                                 ...        ...\n",
       "395  Title: The Pros and Cons of a Four-Day Work We...          1\n",
       "396  \\nWhether summer projects for students should ...          1\n",
       "397  Building self-esteem by accomplishing goals ca...          1\n",
       "398  Title: Minister Winston's Selfless Acts of Kin...          1\n",
       "399  As a grade 7 student, I do not have personal o...          1\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/mytrain.csv')\n",
    "df = df.iloc[:, -2:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['generated'], test_size=0.2, random_state=42)\n",
    "train_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    verbose = 0  # Verbosity\n",
    "    \n",
    "    wandb = True  # Weights & Biases logging\n",
    "    competition = 'llm-detect-ai-generated-text'  # Competition name\n",
    "    _wandb_kernel = 'awsaf49'  # WandB kernel\n",
    "    comment = 'DebertaV3-MaxSeq_200-ext_s-torch'  # Comment description\n",
    "    \n",
    "    preset = \"deberta_v3_base_en\"  # Name of pretrained models\n",
    "    sequence_length = 200  # Input sequence length\n",
    "    \n",
    "    device = 'TPU'  # Device\n",
    "    \n",
    "    seed = 42  # Random seed\n",
    "    \n",
    "    num_folds = 5  # Total folds\n",
    "    selected_folds = [0, 1, 2]  # Folds to train on\n",
    "    \n",
    "    epochs = 3 # Training epochs\n",
    "    batch_size = 3  # Batch size\n",
    "    drop_remainder = True  # Drop incomplete batches\n",
    "    cache = True # Caches data after one iteration, use only with `TPU` to avoid OOM\n",
    "    \n",
    "    scheduler = 'cosine'  # Learning rate scheduler\n",
    "    \n",
    "    class_names = [\"real\", \"fake\"]  # Class names [A, B, C, D, E]\n",
    "    num_classes = len(class_names)  # Number of classes\n",
    "    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]\n",
    "    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping\n",
    "    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/deberta_v3_base_en/v1/vocab.spm\n",
      "2464616/2464616 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
    "    preset=CFG.preset, # Name of the model\n",
    "    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids : (200,)\n",
      "padding_mask : (200,)\n"
     ]
    }
   ],
   "source": [
    "inp = preprocessor(df.text.iloc[0])  # Process text for the first row\n",
    "# Display the shape of each processed output\n",
    "\n",
    "for k, v in inp.items():\n",
    "    print(k, \":\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(text, label=None):\n",
    "    text = preprocessor(text)  # Preprocess text\n",
    "    return (text, label) if label is not None else text  # Return processed text and label if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(texts, labels=None, batch_size=32,\n",
    "                  cache=False, drop_remainder=True,\n",
    "                  repeat=False, shuffle=1024):\n",
    "    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n",
    "    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n",
    "    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n",
    "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n",
    "    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n",
    "    opt = tf.data.Options()  # Create dataset options\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n",
    "        opt.experimental_deterministic = False\n",
    "    ds = ds.with_options(opt)  # Set dataset options\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n",
    "    ds = ds.prefetch(AUTO)  # Prefetch next batch\n",
    "    return ds  # Return the built dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(fold):\n",
    "    train_df = df[df.fold!=fold].sample(frac=1)  # Get training fold data\n",
    "        \n",
    "    train_texts = train_df.text.tolist()  # Extract training texts\n",
    "    train_labels = train_df.label.tolist()  # Extract training labels\n",
    "    \n",
    "    # Build training dataset\n",
    "    train_ds = build_dataset(train_texts, train_labels,\n",
    "                             batch_size=CFG.batch_size*CFG.replicas, cache=CFG.cache,\n",
    "                             shuffle=True, drop_remainder=True, repeat=True)\n",
    "\n",
    "    valid_df = df[df.fold==fold].sample(frac=1)  # Get validation fold data\n",
    "    valid_texts = valid_df.text.tolist()  # Extract validation texts\n",
    "    valid_labels = valid_df.label.tolist()  # Extract validation labels\n",
    "    \n",
    "    # Build validation dataset\n",
    "    valid_ds = build_dataset(valid_texts, valid_labels,\n",
    "                             batch_size=min(CFG.batch_size*CFG.replicas, len(valid_df)), cache=CFG.cache,\n",
    "                             shuffle=False, drop_remainder=True, repeat=False)\n",
    "    \n",
    "    return (train_ds, train_df), (valid_ds, valid_df)  # Return datasets and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Create a DebertaV3Classifier model\n",
    "    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n",
    "        CFG.preset,\n",
    "        preprocessor=None,\n",
    "        num_classes=1 # one output per one option, for five options total 5 outputs\n",
    "    )\n",
    "    inputs = classifier.input\n",
    "    logits = classifier(inputs)\n",
    "        \n",
    "    # Compute final output\n",
    "    outputs = keras.layers.Activation(\"sigmoid\")(logits)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Compile the model with optimizer, loss, and metrics\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(5e-6),\n",
    "        loss=keras.losses.BinaryCrossentropy(label_smoothing=0.02),\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name=\"auc\"),\n",
    "        ],\n",
    "        jit_compile=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.optimizers' has no attribute 'AdamW'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Compile the model with optimizer, loss, and metrics\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m---> 17\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m(\u001b[38;5;241m5e-6\u001b[39m),\n\u001b[0;32m     18\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m),\n\u001b[0;32m     19\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     20\u001b[0m         keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     21\u001b[0m     ],\n\u001b[0;32m     22\u001b[0m     jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.optimizers' has no attribute 'AdamW'"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in CFG.selected_folds:\n",
    "    # Initialize Weights and Biases if enabled\n",
    "    if CFG.wandb:\n",
    "        run = wandb_init(fold)\n",
    "\n",
    "    # Get train and validation datasets\n",
    "    (train_ds, train_df), (valid_ds, valid_df) = get_datasets(fold)\n",
    "    \n",
    "    # Get callback functions for training\n",
    "    callbacks = get_callbacks(fold)\n",
    "\n",
    "    # Print training information\n",
    "    print('#' * 50)\n",
    "    print(f'\\tFold: {fold + 1} | Model: {CFG.preset}\\n\\tBatch Size: {CFG.batch_size * CFG.replicas} | Scheduler: {CFG.scheduler}')\n",
    "    print(f'\\tNum Train: {len(train_df)} | Num Valid: {len(valid_df)}')\n",
    "    print('#' * 50)\n",
    "    \n",
    "    # Clear TensorFlow session and build the model within the strategy scope\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model()\n",
    "\n",
    "    # Start training the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=int(len(train_df) / CFG.batch_size / CFG.replicas),\n",
    "    )\n",
    "    \n",
    "    # Find the epoch with the best validation accuracy\n",
    "    best_epoch = np.argmax(model.history.history['val_auc'])\n",
    "    best_auc = model.history.history['val_auc'][best_epoch]\n",
    "    best_loss = model.history.history['val_loss'][best_epoch]\n",
    "\n",
    "    # Print and display best results\n",
    "    print(f'\\n{\"=\" * 17} FOLD {fold} RESULTS {\"=\" * 17}')\n",
    "    print(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST AUC   : {best_auc:.3f}\\n>>>> BEST Epoch : {best_epoch}')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    # Log best result on Weights and Biases (wandb) if enabled\n",
    "    if CFG.wandb:\n",
    "        log_wandb()  # Log results\n",
    "        wandb.run.finish()  # Finish the run\n",
    "#         display(ipd.IFrame(run.url, width=1080, height=720)) # show wandb dashboard\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
